# Liandrin : Time-travelling Conversational AI Assistant

This project is a real-time voice-based conversational AI assistant built with FastAPI, WebSockets, and modern AI services. It lets users speak with the assistant using their microphone, transcribes speech in real-time, generates AI-driven responses, and streams back natural-sounding audio responses.
Think of it as a two-way voice conversation with an AI, powered by:
AssemblyAI for real-time transcription (STT)
Google Gemini for large language model (LLM) responses
Murf for text-to-speech (TTS)
News API + SerpAPI (optional integrations for context/data retrieval)
The entire pipeline runs in real-time, handling audio streaming, AI response generation, and audio playback seamlessly.

## 🚀 Features

✅ Web-based Interface – Works directly in the browser with no installations required.</br>
✅ API Key Management – Securely set and store API keys from the UI.</br>
✅ Real-Time Speech Recognition – Streams audio to AssemblyAI for transcription.</br>
✅ Conversational AI – Sends transcribed text to a Gemini LLM for natural language responses.</br>
✅ Text-to-Speech Streaming – Uses Murf to generate realistic AI voice responses.</br>
✅ WebSocket-based Streaming – Full-duplex communication between client and server.</br>
✅ Fallback Handling – Provides pre-recorded fallback audio if APIs are unavailable.</br>
✅ Chat History – Keeps track of past messages for context-aware conversations.</br>

## 🏗️ Architecture

Frontend (script.js + index.html)</br>
Captures microphone input via Web Audio API</br>
Streams PCM audio over WebSocket (/ws)</br>
Displays live transcriptions & AI responses</br>
Queues and plays back streamed audio chunks</br>
Backend (FastAPI - main.py)</br>

```/set_keys``` → Save API keys to .env file

```/agent/chat``` → STT → LLM → TTS pipeline for one-shot requests

```/ws``` → Real-time bi-directional streaming (STT + LLM + TTS)

```/tts```& ```/voices``` → Direct TTS endpoints

Uses AssemblyAI StreamingClient for live transcription

Runs LLM + TTS pipeline per user session

External Services

AssemblyAI → Speech-to-Text (real-time streaming)

Google Gemini → Conversational responses

Murf → Voice generation (TTS)

News API + SerpAPI → Optional retrieval augmentation

## 📂 Project Structure
```
.
├── main.py                # FastAPI app (backend logic)
├── services/              # STT, LLM, and TTS service wrappers
├── schemas/               # Pydantic request/response schemas
├── templates/
│   └── index.html         # Frontend UI
├── static/
│   ├── script.js          # Client-side logic
│   └── fallback.mp3       # Fallback audio
├── uploads/               # Saved audio streams
├── config.py              # Configuration (env vars loaded here)
├── .env                   # API keys (autogenerated)
└── README.md              # Documentation
```
## ⚙️ Setup & Installation</br>
1️⃣ Clone the Repository</br>
```
git clone https://github.com/vignesh-naik-720/Liandrin.git
cd Liandrin
```
2️⃣ Create Virtual Environment & Install Dependencies</br>
```
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)

pip install -r requirements.txt
```
3️⃣ Set Up Environment Variables

API keys can be set in two ways:

Option A (recommended) → Enter keys in the frontend modal (stored in .env)

Option B → Manually create .env file:
```
MURF_API_KEY=your_murf_key
ASSEMBLYAI_API_KEY=your_assemblyai_key
GEMINI_API_KEY=your_gemini_key
NEWS_API_KEY=your_news_key
SERP_API_KEY=your_serp_key
```
4️⃣ Run the Server
```
uvicorn main:app --reload
```

Now visit ```http://127.0.0.1:8000```
 in your browser.

## 🎤 Usage

Open the app in your browser.

Enter your API keys in the modal (one-time setup).

Click Record 🎙️ and start speaking.

Watch your speech transcription appear in real-time.

The AI assistant will generate a response and speak back.

Continue the conversation naturally.

## Live Demo:
Link: https://liandrin.onrender.com/

## 🛡️ Security Notes

API keys are saved in .env and never exposed to frontend after initial input.

WebSocket connection is session-specific (unique session_id per client).

Audio files are stored in /uploads but can be auto-cleaned in production.

For production:

Use HTTPS (wss://)

Rotate API keys regularly

Consider rate-limiting

## 🛠️ Tech Stack

Backend: FastAPI + WebSockets

Frontend: Vanilla JS + Web Audio API

AI Services: AssemblyAI, Google Gemini, Murf, SerpAPI, News API

Infra: Uvicorn

## 📌 Future Enhancements

✅ Add user authentication</br>

✅ Store chat history in database (SQLite/Postgres)</br>

✅ Deploy with Docker + Render/Heroku</br>

✅ Add support for multiple TTS providers (e.g., ElevenLabs)</br>

✅ Add streaming avatars with lip-sync</br>
